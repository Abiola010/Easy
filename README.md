Globally, tech companies are battling the recession. Consumer spending is slowing down, central banks are raising interest rates, and the foreign exchange rate is strong, all of which point to a potential recession, and IT companies have already begun to lay off staff. Due to the current economic downturn, Meta laid off 13% of its staff or more than 11,000 workers. Upon completion of my data alaysis training this dataset from Kaggle.com was choosen to analysis using Microsoft Excel.

CONCERNING THE DATASET
The dataset, a Comma Separated Format (CSV) file retrieved from Kaggle (https://www.kaggle.com/datasets/swaptr/layoffs-2022) covers layoffs of workers across numerous businesses from COVID-19 until the present. The dataset has nine columns: business name of the firm, location of the layoff, industry of the firm, total number of workers laid off), percentage laid off of employees, date of the layoff, stages of funding), Country, and funds raised by the firms.
 

Essentials of Data cleaning 
Businesses that desire to dominate their markets must understand where to obtain the data they want and how it all relates. However, they must ensure that their data sets are clean before beginning to analyze data. Data cleansing is undoubtedly important, and smart businesses are aware of this. Large amounts of data, sometimes stored in hard-to-use forms, are typically included in datasets. Data analysts must first ensure that the data is adequately equipped and follows the established set of criteria. The greatest problems are data scarcity and inconsistent formatting, and data cleaning addresses these issues. Data cleaning is the process of locating faulty, incomplete, inaccurate, or irrelevant data, fixing the flaws, and ensuring that any future instances of these problems will be automatically corrected.

Data analysts spend 60% of their time organizing and cleaning up data, according to Appen!

Here are a few of the most popular stages and techniques for data purification that seasoned development teams recommend:
•	Dealing with missing data
•	Standardizing the process
•	Validating data accuracy
•	Removing duplicate data
•	Handling structural errors
•	Getting rid of unwanted observations

